{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:66ch;\">\n",
    "\n",
    "## 0. Glossary (\\*)\n",
    "\n",
    "You can use an LLM for help and/or search for results online, but try to summarize the meaning in your own words, in order to properly learn the terminologies. I don't expect you to know all of them now, but after this course you should know these and many more by heart.\n",
    "\n",
    "| Terminology                | Meaning                                                                      |\n",
    "| :------------------------- | ------:                                                                      |\n",
    "| supervised learning        | Redan satta exempel som den kan jämföra mot för facit.                       |\n",
    "| unsupervised learning      | Försöker hitta egna exempel och mönster.                                     |\n",
    "| machine learning           | Algoritmer och modeller som kan säga vad något borde vara (Pichu & Pikachu). |\n",
    "| deep learning              | Använder artificiella neurala nätverk som kan lära digitala system att fatta beslut baserat på ostrukturerad omärkt data.                       |\n",
    "| pattern recognition        | Hittar mönster i data och använder machine Learning.                         |\n",
    "| reinforcement learning     | Machine Learning där en maskin lär sig ta beslut beroende på sin omgivning. Belöning och straff.    |\n",
    "| data science               | Paraplybegrepp över allt det här.                     |\n",
    "| data engineering           | Organiserar, tvättar och tillgängligör stora mängder data som kan användas för analys eller som input-data        |\n",
    "| computer vision            | Analyserar pixlar i bilder (typ välj alla stoppljus för att verifiera att du inte är en robot.)                   |\n",
    "| algorithm                  | Regler och instruktioner för att lösa ett problem. Används för att träna modeller med data och lunna förutspå eller ta beslut.        |\n",
    "| bias                       |                      |\n",
    "| variance                   |                      |\n",
    "| overfitting                |                      |\n",
    "| underfitting               |                      |\n",
    "| gradient descent           |                      |\n",
    "| transfer learning          |                      |\n",
    "| regression                 |                      |\n",
    "| classification             |                      |\n",
    "| artificial neural networks |                      |\n",
    "| data augmentation          |                      |\n",
    "| synthetic data             |                      |\n",
    "| regularization             |                      |\n",
    "| perceptron                 |                      |\n",
    "| qualitative data           |                      |\n",
    "| quantitative data          |                      |\n",
    "| independent variable       |                      |\n",
    "| dependent variable         |                      |\n",
    "| label                      |                      |\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "Draw an illustration of how machine learning, deep learning and artificial intelligence relate to each other and explain it with your own words.\n",
    "\n",
    "---\n",
    "Artificial Intelligence (AI):\n",
    "\n",
    "- AI is the broadest concept, referring to the simulation of human intelligence processes by machines. It encompasses any technique that enables computers to mimic human cognitive functions such as learning, problem-solving, understanding natural language, and recognizing patterns.\n",
    "\n",
    "Machine Learning (ML):\n",
    "\n",
    "- Machine learning is a subset of AI focused on the development of algorithms and models that enable computers to learn from data and improve their performance on a specific task without being explicitly programmed. ML algorithms use statistical techniques to identify patterns in data and make predictions or decisions.\n",
    "\n",
    "Deep Learning (DL):\n",
    "\n",
    "- Deep learning is a subfield of machine learning that utilizes artificial neural networks with multiple layers (hence the term \"deep\") to learn representations of data. These neural networks are inspired by the structure and function of the human brain. Deep learning algorithms can automatically discover intricate patterns and features from large amounts of unstructured data, such as images, audio, and text.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- Artificial Intelligence (AI) serves as the overarching concept, representing the quest to create machines that can perform tasks that typically require human intelligence.\n",
    "\n",
    "- Machine Learning (ML) is a specialized technique within AI that enables computers to learn from data and improve their performance over time without explicit programming. It's like teaching a computer to recognize patterns and make decisions based on examples.\n",
    "\n",
    "- Deep Learning (DL) is a subset of machine learning that focuses on using artificial neural networks to learn intricate patterns from vast amounts of data. Deep learning models, inspired by the structure of the human brain, are particularly powerful for tasks involving complex data like images, audio, and natural language processing.\n",
    "\n",
    "In summary, artificial intelligence encompasses the broader goal of creating intelligent machines, machine learning provides the techniques for machines to learn from data, and deep learning offers a specific approach within machine learning that uses neural networks to learn complex patterns. Each field builds upon the other, contributing to advancements in technology and the realization of intelligent systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "What is the main difference between regression and classification? \n",
    "\n",
    "a) Give an example of a problem that can be solved with regression\n",
    "\n",
    "b) Give an example of a problem that can be solved with classification\n",
    "\n",
    "---\n",
    "\n",
    "The main difference between regression and classification lies in the type of output they produce:\n",
    "\n",
    "a) Regression:\n",
    "\n",
    "- Regression is a type of supervised learning where the model predicts continuous numerical values. It aims to find the relationship between independent variables (features) and dependent variables (target) in the data.\n",
    "Example: Predicting house prices based on features such as area, number of bedrooms, location, etc. Here, the target variable (house price) is continuous and can take any numerical value.\n",
    "\n",
    "b) Classification:\n",
    "\n",
    "- Classification is also a type of supervised learning, but it predicts categorical outcomes or class labels. The model categorizes input data into predefined classes or categories.\n",
    "Example: Email spam detection, where the model classifies emails into either \"spam\" or \"not spam\" categories based on various features like the subject line, sender, and content. Here, the output is discrete and falls into distinct categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "\n",
    "What does scaling data mean, and why do some machine learning algorithm require data to be scaled?\n",
    "\n",
    "---\n",
    "\n",
    "Scaling data refers to the process of transforming the values of numerical features in a dataset to a similar scale. This is typically done to ensure that all features contribute equally to the analysis, especially when using machine learning algorithms. Scaling helps in standardizing the range of features so that they have a similar impact on the model training process and prevents features with large scales from dominating those with smaller scales.\n",
    "\n",
    "Some machine learning algorithms, such as support vector machines (SVM), K-nearest neighbors (KNN), and gradient descent-based optimization algorithms (e.g., logistic regression, neural networks), require data to be scaled for the following reasons:\n",
    "\n",
    "- Distance-based algorithms: Algorithms like KNN and SVM calculate distances between data points. If the features are not scaled, features with larger scales may influence the distance metric more than features with smaller scales, leading to biased results.\n",
    "\n",
    "- Gradient-based optimization: Algorithms that involve gradient descent, such as neural networks and logistic regression, converge faster when features are on a similar scale. This is because large-scale features can cause the gradient updates to oscillate or take longer to converge.\n",
    "\n",
    "- Regularization: Regularization techniques, such as L1 and L2 regularization, penalize large coefficients. Scaling the features ensures that all features are penalized equally, preventing certain features from being unfairly favored in the regularization process.\n",
    "\n",
    "By scaling the data, we ensure that the model learns optimal parameters in a fair and efficient manner, leading to better performance and more reliable predictions. Common scaling techniques include min-max scaling, standardization (z-score normalization), and robust scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "\n",
    "What is the purpose to split the data into a training part and a test part? \n",
    "\n",
    "---\n",
    "Splitting the data into training and test sets is a fundamental step in machine learning model development, and it serves several important purposes:\n",
    "\n",
    "- Evaluation of Model Performance: The primary purpose of splitting the data is to evaluate the performance of the trained model on unseen data. By reserving a portion of the dataset for testing, we can assess how well the model generalizes to new, unseen samples. This helps us understand if the model has learned meaningful patterns from the training data or if it is simply memorizing the training examples (overfitting).\n",
    "\n",
    "- Prevention of Overfitting: Overfitting occurs when a model learns to perform well on the training data but fails to generalize to new data. By evaluating the model on a separate test set, we can detect if the model is overfitting. If the model performs well on the training set but poorly on the test set, it indicates overfitting, and adjustments to the model, such as regularization or feature selection, may be necessary to improve generalization.\n",
    "\n",
    "- Hyperparameter Tuning: Splitting the data allows for the tuning of model hyperparameters. Hyperparameters are configuration settings that are not learned from the data but affect the learning process and model performance. Techniques like cross-validation can be used on the training set to tune hyperparameters while still preserving the test set for final evaluation.\n",
    "\n",
    "- Avoiding Data Leakage: Keeping a separate test set helps prevent data leakage, where information from the test set inadvertently influences model training. If the test set were combined with the training set, the model might inadvertently learn patterns specific to the test set, leading to inflated performance estimates.\n",
    "\n",
    "Overall, splitting the data into training and test sets enables the development of machine learning models that generalize well to new, unseen data, providing a more accurate assessment of model performance and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\n",
    "\n",
    "What is data leakage, why is it bad and how can you avoid it?\n",
    "\n",
    "---\n",
    "Data leakage refers to the inadvertent inclusion of information from the test set or other external sources into the training process of a machine learning model. This can lead to overly optimistic performance estimates during model development, as the model may inadvertently learn patterns or information that it would not have access to when applied to new, unseen data. Data leakage is considered bad because it compromises the reliability and generalization ability of the model, leading to poor performance when deployed in real-world scenarios.\n",
    "\n",
    "Data leakage can occur in various ways, including:\n",
    "\n",
    "- Using Future Information: Including information that would not be available at the time of prediction, such as using future timestamps or events to make predictions.\n",
    "\n",
    "- Incorporating Test Set Information: Utilizing information from the test set during model training, such as using features derived from the test set or tuning model hyperparameters based on test set performance.\n",
    "\n",
    "- Data Preprocessing: Performing data preprocessing steps (e.g., feature scaling, imputation) based on the entire dataset before splitting it into training and test sets, which can lead to information leakage.\n",
    "\n",
    "To avoid data leakage and ensure the integrity of the model evaluation process, it's essential to follow best practices:\n",
    "\n",
    "- Split Data Properly: Always split the dataset into distinct training and test sets before any preprocessing or model training occurs. Ensure that the test set remains entirely untouched during model development and evaluation.\n",
    "\n",
    "- Use Cross-Validation: When hyperparameter tuning or model selection is performed, use techniques like k-fold cross-validation on the training set. This ensures that model evaluation metrics are based on truly unseen data and helps prevent overfitting to the training set.\n",
    "\n",
    "- Feature Engineering: Be cautious when creating new features or deriving features from the dataset. Ensure that all feature engineering steps are performed solely on the training data to avoid incorporating information from the test set.\n",
    "\n",
    "- Feature Selection: If feature selection techniques are employed, such as filtering or wrapper methods, ensure that they are applied only to the training data to prevent information leakage from the test set.\n",
    "\n",
    "- Regular Validation: Regularly validate the model's performance on the test set to detect any unexpected drops in performance, which may indicate data leakage or other issues.\n",
    "\n",
    "By following these practices, data leakage can be mitigated, ensuring that machine learning models generalize well to new data and produce reliable predictions in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "\n",
    "How does cross-validation work, and when is it good to use cross-validation?\n",
    "\n",
    "---\n",
    "Cross-validation is a technique used to assess how well a predictive model will generalize to an independent dataset. It involves partitioning the dataset into multiple subsets, called folds, training the model on a subset of the data, and then evaluating its performance on the remaining data. This process is repeated multiple times, each time using a different subset as the test set and the remaining subsets as the training set. The performance metrics obtained from each fold are then averaged to provide a more robust estimate of the model's performance.\n",
    "\n",
    "Here's how cross-validation typically works:\n",
    "\n",
    "- Partition the Data: The dataset is divided into k equal-sized folds.\n",
    "\n",
    "- Training and Testing: For each iteration, one fold is held out as the test set, and the remaining k-1 folds are used as the training set.\n",
    "\n",
    "- Model Training: The model is trained on the training set, and its performance is evaluated on the test set using a chosen evaluation metric (e.g., accuracy, mean squared error).\n",
    "\n",
    "- Performance Evaluation: The performance metric obtained from each iteration is recorded.\n",
    "\n",
    "- Average Performance: After all iterations are complete, the performance metrics from each fold are averaged to obtain a single performance estimate for the model.\n",
    "\n",
    "Cross-validation is good to use in various situations:\n",
    "\n",
    "- Model Evaluation: It provides a more accurate estimate of a model's performance compared to a single train-test split, especially when the dataset is small or imbalanced.\n",
    "\n",
    "- Hyperparameter Tuning: Cross-validation is commonly used for hyperparameter tuning, where different combinations of model parameters are evaluated using cross-validation to identify the optimal set of hyperparameters.\n",
    "\n",
    "- Model Selection: Cross-validation helps in comparing the performance of multiple models to select the best-performing one for the given dataset.\n",
    "\n",
    "- Feature Selection: Cross-validation can be used to assess the impact of different sets of features on the model's performance and aid in feature selection.\n",
    "\n",
    "- Assessing Model Stability: It helps in assessing the stability of a model by evaluating its performance across multiple subsets of the data.\n",
    "\n",
    "Overall, cross-validation is a valuable technique for assessing and improving the performance of machine learning models, ensuring that they generalize well to unseen data and providing more reliable estimates of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "When can you and should you use a confusion matrix? \n",
    "\n",
    "---\n",
    "A confusion matrix is a useful tool for evaluating the performance of classification models. It provides a summary of the predictions made by the model compared to the actual labels in the dataset.\n",
    "\n",
    "You can and should use a confusion matrix in the following situations:\n",
    "\n",
    "- Model Evaluation: When you want to assess the performance of a classification model, a confusion matrix gives you a clear picture of how well the model is performing across different classes. It allows you to identify where the model is making errors, such as misclassifying certain classes more frequently than others.\n",
    "\n",
    "- Class Imbalance: In datasets where there is a significant class imbalance, meaning some classes have much fewer samples than others, a confusion matrix helps you understand how well the model is performing for each class. This is important because accuracy alone may not be a reliable metric in such scenarios.\n",
    "\n",
    "- Model Comparison: When comparing multiple classification models or different configurations of the same model, a confusion matrix provides a detailed comparison of their performance across different classes. This allows you to choose the model that performs best for your specific requirements.\n",
    "\n",
    "- Threshold Selection: In binary classification problems where the model's output needs to be converted into class labels using a threshold (e.g., probability threshold for logistic regression), a confusion matrix can help you choose an appropriate threshold by showing the trade-offs between false positives and false negatives.\n",
    "\n",
    "- Identifying Errors: By analyzing the confusion matrix, you can identify specific types of errors made by the model, such as false positives (Type I errors) and false negatives (Type II errors). This insight can guide further improvements to the model, such as feature engineering or adjusting the model's parameters.\n",
    "\n",
    "Overall, a confusion matrix is a versatile tool that provides valuable insights into the performance of classification models and helps guide decision-making in various stages of the machine learning workflow, from model development to deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
